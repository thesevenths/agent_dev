import os
from process_report import process_mds_to_json, load_items_from_json  # ä¿®æ”¹ä¸ºmds
from retrievers import get_bm25_retriever, get_vector_retriever
from query_engine import build_query_engine
from pathlib import Path
import pickle

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from dotenv import load_dotenv
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

from config import LLAMA_CLOUD_API_KEY
from config import DASHSCOPE_API_KEY
from config import MD_DIR, JSON_DIR  # æ–°å¢ä»configå¯¼å…¥

# å°è¯•å¯¼å…¥å®˜æ–¹ Documentï¼›è‹¥å¤±è´¥åˆ™ç”¨ process_report çš„å›é€€ç‰ˆæœ¬
try:
    from llama_index.core import Document as OfficalDocument
except Exception:
    OfficalDocument = None

CHROMA_PATH = "E:\\model\\RAG\\chroma_db"
NODES_CACHE = "E:\\model\\RAG\\nodes.pkl"

def _convert_documents(docs):
    """
    å°† process_report.Document æˆ–å…¶å®ƒ Document è½¬æ¢ä¸ºå®˜æ–¹ llama-index Documentï¼ˆè‹¥å¯ç”¨ï¼‰ã€‚
    è‹¥å®˜æ–¹ Document ä¸å¯ç”¨åˆ™ä¿æŒåŸæ ·ã€‚
    """
    if OfficalDocument is None:
        return docs
    
    converted = []
    for d in docs:
        # è‹¥å·²æ˜¯å®˜æ–¹ Documentï¼Œè·³è¿‡
        if isinstance(d, OfficalDocument):
            converted.append(d)
        else:
            # è½¬æ¢ï¼šä»å›é€€ Document æå– text å’Œ metadata
            txt = getattr(d, "text", "") or ""
            meta = getattr(d, "metadata", {}) or {}
            converted.append(OfficalDocument(text=txt, metadata=meta))
    return converted

def main():
    # 1) å…ˆæŠŠ MD è½¬ä¸º jsonï¼ˆæœ‰ç¼“å­˜åˆ™è·³è¿‡ï¼‰
    print("1. Converting MDs to JSON (cached)...")
    json_paths = process_mds_to_json(MD_DIR, json_dir=JSON_DIR, force=False)
    print(f" -> {len(json_paths)} json files ready in {JSON_DIR}")

    # 2) ä» json åŠ è½½ itemsï¼ˆæ¯ä¸ª text chunk ä¸æ¯ä¸ª table éƒ½å˜æˆä¸€ä¸ª Documentï¼‰
    has_nodes = os.path.exists(NODES_CACHE)
    if has_nodes:
        print("ğŸ” Loading documents cache from nodes.pkl...")
        with open(NODES_CACHE, "rb") as f:
            documents = pickle.load(f)
    else:
        print("2. Loading items from JSON into Documents...")
        documents = load_items_from_json(JSON_DIR)
        with open(NODES_CACHE, "wb") as f:
            pickle.dump(documents, f)
        print(f"âœ… Cached documents to {NODES_CACHE} (total {len(documents)})")

    # è½¬æ¢ä¸ºå®˜æ–¹ Documentï¼ˆå¦‚æœå¯ç”¨ï¼‰
    documents = _convert_documents(documents)

    print("3. Building retrievers...")
    # å‘é‡æ£€ç´¢å™¨ï¼šè‡ªåŠ¨å¤„ç† Chroma æŒä¹…åŒ–ï¼ˆåœ¨ retrievers.py ä¸­å®ç°ï¼‰
    vector_retriever = get_vector_retriever(documents=documents, top_k=25)

    # BM25 æ£€ç´¢å™¨ï¼šç”¨ documentsï¼ˆæ¯ä¸ª item ä¸€ä¸ª Documentï¼‰
    bm25_retriever = get_bm25_retriever(documents=documents, top_k=30)

    print("âœ… RAG system ready!")
    while True:
        query = input("\nYour question (or 'quit'): ").strip()
        if query.lower() == 'quit':
            break
        query_engine = build_query_engine(bm25_retriever, vector_retriever, query)
        response = query_engine.query(query)
        print("\nAnswer:", response.response)
        print("\nSources:")
        for i, node in enumerate(response.source_nodes, 1):
            meta = getattr(node.node, "metadata", {}) if hasattr(node, "node") else getattr(node, "metadata", {})
            print(f"{i}. {meta.get('source', 'Unknown')} (table={meta.get('is_table', False)})")
            preview = (getattr(node.node, "text", "") if hasattr(node, "node") else getattr(node, "text", ""))[:150].replace("\n", " ")
            print(f"   Preview: {preview}...")

if __name__ == "__main__":
    main()