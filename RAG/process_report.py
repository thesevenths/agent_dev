import os
import re
import pandas as pd
import json
from pathlib import Path
from typing import List, Dict
import markdown  # æ–°å¢ï¼šç”¨äºè§£æMD
import io  # ç”¨äºpandasè¯»å–MDè¡¨æ ¼
import jieba  # ç”¨äºtokenè®¡æ•°å’Œåˆ†è¯

# å°è¯•å¯¼å…¥ Documentï¼ˆå…¼å®¹ä¸åŒ llama-index ç‰ˆæœ¬ï¼‰ï¼Œè‹¥ä¸å¯ç”¨åˆ™æä¾›ç®€å•å›é€€
try:
    from llama_index import Document
except Exception:
    try:
        from llama_index.schema import Document
    except Exception:
        try:
            from llama_index.node import Document
        except Exception:
            import uuid, hashlib, json
            from dataclasses import dataclass, field
            from typing import Any, Dict

            @dataclass
            class Document:
                text: str
                metadata: dict = field(default_factory=dict)
                id_: str = None
                hash: str = None
                doc_id: str = None

                def __post_init__(self):
                    if not self.id_:
                        self.id_ = (
                            self.metadata.get("item_id")
                            or self.metadata.get("table_id")
                            or self.metadata.get("source")
                            or self.doc_id
                            or str(uuid.uuid4())
                        )
                    if not self.doc_id:
                        self.doc_id = self.id_
                    if not self.hash:
                        meta_str = json.dumps(self.metadata or {}, sort_keys=True, ensure_ascii=False)
                        h = hashlib.md5()
                        h.update((self.text or "").encode("utf-8"))
                        h.update(meta_str.encode("utf-8"))
                        self.hash = h.hexdigest()

                def model_dump(self, mode: str = "json") -> Dict[str, Any]:
                    out = dict(self.metadata) if isinstance(self.metadata, dict) else {}
                    out.update({"text": self.text, "id_": self.id_, "doc_id": self.doc_id})
                    return out

                def get_metadata_str(self, mode=None, **kwargs) -> str:
                    return json.dumps(self.metadata or {}, ensure_ascii=False, sort_keys=True)

                def get_text(self, *args, **kwargs) -> str:
                    return self.text

                def get_content(self, metadata_mode=None, *args, **kwargs) -> str:
                    return self.text

                def class_name(self) -> str:
                    return self.__class__.__name__

                def as_related_node_info(self) -> Dict[str, Any]:
                    """
                    è¿”å›ä¸€ä¸ªç”¨äºæ„å»ºå…³ç³»çš„ç®€å•ç»“æ„ã€‚
                    llama-index æœŸæœ›æœ‰ as_related_node_info()ï¼Œè¿™é‡Œè¿”å›å¸¸è§å­—æ®µçš„å­—å…¸ã€‚
                    """
                    return {
                        "doc_id": self.id_,
                        "node_id": self.id_,
                        "extra_info": dict(self.metadata or {})
                    }

                def __repr__(self):
                    return f"<FallbackDocument id_={self.id_} source={self.metadata.get('source')}>"

# ========== é…ç½® ==========
from config import MD_DIR, JSON_DIR  # ä»configå¯¼å…¥
Path(JSON_DIR).mkdir(parents=True, exist_ok=True)

CHUNK_SIZE = 500  # tokenæ•°ï¼ˆç”¨jiebaè¯æ•°è¿‘ä¼¼ï¼‰
OVERLAP = 50

def clean_text(text: str):
    """æ¸…ç†æ¢è¡Œç­‰æ‚å­—ç¬¦"""
    return re.sub(r'\s+', ' ', text).strip()

def count_tokens(text: str) -> int:
    """ç”¨jiebaåˆ†è¯è®¡æ•°ä½œä¸ºtokenè¿‘ä¼¼"""
    return len(jieba.lcut(text))

def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:
    """å°†æ–‡æœ¬chunkæˆæŒ‡å®šå¤§å°ï¼Œoverlap"""
    words = jieba.lcut(text)
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = ' '.join(words[start:end])
        chunks.append(chunk)
        start = end - overlap
        if end >= len(words):
            break
    return chunks

def extract_tables_from_md(md_content: str) -> List[pd.DataFrame]:
    """ä»MDæå–è¡¨æ ¼å—ï¼Œè¿”å›list of DataFrame"""
    tables = []
    # åŒ¹é…MDè¡¨æ ¼ï¼šè‡³å°‘æœ‰headerå’Œdivider
    table_pattern = r'(\|.*?\n\|[-:\s\|]+\n(?:\|.*?\n)+)'
    for match in re.finditer(table_pattern, md_content, re.MULTILINE):
        table_str = match.group(1)
        try:
            df = pd.read_csv(io.StringIO(table_str), sep='|', engine='python').dropna(how='all', axis=1)
            df.columns = df.columns.str.strip()
            tables.append(df)
        except:
            pass  # å¿½ç•¥è§£æå¤±è´¥çš„è¡¨æ ¼
    return tables

def process_md(md_path: Path) -> Dict:
    """
    è§£æå•ä¸ªMDï¼Œæå–æ–‡æœ¬å’Œè¡¨æ ¼ï¼Œè¿”å›JSONç»“æ„ï¼ˆdictï¼‰
    """
    result = {
        "file_name": md_path.name,
        "text_chunks": [],  # list of {"content": chunk_text} ï¼ˆå·²chunkï¼‰
        "tables": {}       # key -> {row:col: value, ...}
    }

    try:
        md_content = md_path.read_text(encoding="utf-8")
        filename_prefix = f"æ–‡ä»¶å: {md_path.name}\n"

        # è½¬æ¢ä¸ºHTMLä»¥è¾…åŠ©æå–çº¯æ–‡æœ¬ï¼ˆå»é™¤è¡¨æ ¼ï¼‰
        html = markdown.markdown(md_content)
        # ä½†å®é™…æˆ‘ä»¬ç”¨æ­£åˆ™å»é™¤è¡¨æ ¼å—ï¼Œå‰©ä½™ä¸ºæ–‡æœ¬
        non_table_text = re.sub(r'(\|.*?\n\|[-:\s\|]+\n(?:\|.*?\n)+)', '', md_content, flags=re.MULTILINE)
        non_table_text = clean_text(non_table_text)

        # chunkéè¡¨æ ¼æ–‡æœ¬
        if non_table_text:
            chunks = chunk_text(non_table_text)
            for idx, chunk in enumerate(chunks):
                chunk_with_prefix = filename_prefix + chunk
                result["text_chunks"].append({
                    "chunk_id": idx + 1,
                    "content": chunk_with_prefix
                })

        # æå–è¡¨æ ¼
        tables = extract_tables_from_md(md_content)
        for idx, df in enumerate(tables, 1):
            table_json = table_dataframe_to_json(df)
            table_text = _serialize_table(table_json)
            table_tokens = count_tokens(table_text)
            key = f"{md_path.stem}_table_{idx}"

            if table_tokens > CHUNK_SIZE:
                # åˆ†æˆä¸¤ä¸ªchunkï¼šå¤§è‡´äºŒåˆ†è¡Œ
                mid = len(df) // 2
                df1 = df.iloc[:mid]
                df2 = df.iloc[mid:]
                table_json1 = table_dataframe_to_json(df1)
                table_json2 = table_dataframe_to_json(df2)
                result["tables"][f"{key}_part1"] = table_json1
                result["tables"][f"{key}_part2"] = table_json2
            else:
                result["tables"][key] = table_json

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥ {md_path.name}: {e}")
        return None

    return result

def table_dataframe_to_json(df: pd.DataFrame) -> Dict[str, str]:
    """
    æŠŠ DataFrame è½¬ä¸º { "row:col": value, ... } ç»“æ„
    """
    table_json: Dict[str, str] = {}
    if df.shape[0] < 2:
        return table_json

    headers = df.iloc[0].fillna("").tolist()
    body = df.iloc[1:].fillna("").values.tolist()

    for row in body:
        row_label = str(row[0]).strip()
        if not row_label:
            continue
        for col_idx in range(1, len(headers)):
            col_label = str(headers[col_idx]).strip()
            raw_val = str(row[col_idx]).strip()
            # è¿‡æ»¤æ‰æ— ç”¨å­—ç¬¦
            if raw_val:
                key = f"{row_label}:{col_label}"
                table_json[key] = raw_val

    return table_json

def _serialize_table(table) -> str:
    """
    å°†è¡¨æ ¼å¯¹è±¡åºåˆ—åŒ–ä¸ºå•å­—ç¬¦ä¸²ï¼ˆä¸å¯æ‹†åˆ†å•å…ƒï¼‰
    """
    if table is None:
        return ""
    if isinstance(table, dict):
        lines = []
        for k, v in table.items():
            lines.append(f"{k}\t{v}")
        return "\n".join(lines)
    try:
        rows = []
        for r in table:
            rows.append("\t".join([str(c) for c in r]))
        return "\n".join(rows)
    except Exception:
        return str(table)


def process_mds_to_json(md_dir: str = None, json_dir: str = JSON_DIR, force: bool = False) -> List[str]:
    """
    éå† md_dir ä¸‹çš„æ‰€æœ‰ mdï¼Œé€ä¸ªè°ƒç”¨ process_mdï¼ŒæŠŠç»“æœä¿å­˜åˆ° json_dirï¼ˆæŒ‰æ–‡ä»¶å .jsonï¼‰ã€‚
    æ”¯æŒåŸºäºä¿®æ”¹æ—¶é—´è·³è¿‡å·²å­˜åœ¨çš„ jsonï¼ˆé™¤é force=Trueï¼‰ã€‚
    è¿”å›å·²ç”Ÿæˆï¼ˆæˆ–å­˜åœ¨ï¼‰çš„ json æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
    """
    md_dir = Path(md_dir or MD_DIR)
    json_dir = Path(json_dir or JSON_DIR)
    json_dir.mkdir(parents=True, exist_ok=True)

    if not md_dir.exists():
        print(f"âŒ MD ç›®å½•ä¸å­˜åœ¨: {md_dir}")
        return []

    md_files = list(md_dir.glob("*.md"))
    if not md_files:
        print(f"âš ï¸ {md_dir} ä¸‹æ²¡æœ‰ MD æ–‡ä»¶")
        return []

    json_paths: List[str] = []
    for md_file in md_files:
        out_file = json_dir / f"{md_file.stem}.json"
        need_write = force
        try:
            if out_file.exists():
                # å¦‚æœ json æ¯” md æ–°ä¸”é forceï¼Œåˆ™è·³è¿‡
                if not force and out_file.stat().st_mtime >= md_file.stat().st_mtime:
                    json_paths.append(str(out_file))
                    print(f"â­ è·³è¿‡ï¼ˆå·²ç¼“å­˜ï¼‰ï¼š{out_file.name}")
                    continue
            need_write = True
        except Exception:
            need_write = True

        print(f"ğŸ“„ å¤„ç†: {md_file.name}")
        json_data = process_md(md_file)
        if json_data is None:
            continue

        with open(out_file, "w", encoding="utf-8") as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        json_paths.append(str(out_file))
        print(f"âœ… å·²ä¿å­˜ JSON: {out_file.name}")

    return json_paths

def load_items_from_json(json_dir: str = JSON_DIR) -> List[Document]:
    """
    ä» json_dir è¯»å–æ‰€æœ‰ json æ–‡ä»¶ï¼Œè¿”å› llama_index.Document åˆ—è¡¨ã€‚
    æ¯ä¸ª text chunk -> ä¸€ä¸ª Documentï¼›æ¯ä¸ª table -> ä¸€ä¸ª Documentï¼ˆis_table metadataï¼‰
    """
    docs: List[Document] = []
    json_dir = Path(json_dir or JSON_DIR)
    for p in json_dir.glob("*.json"):
        try:
            obj = json.loads(p.read_text(encoding="utf-8"))
        except Exception:
            continue
        source = obj.get("file_name", p.stem)
        # text chunks
        for t in obj.get("text_chunks", []):
            meta = {"source": source, "is_table": False}
            docs.append(Document(text=t.get("content", ""), metadata=meta))
        # tables
        for tbl_key, tbl in obj.get("tables", {}).items():
            meta = {"source": source, "table_id": tbl_key, "is_table": True}
            filename_prefix = f"æ–‡ä»¶å: {source}\n"
            txt = filename_prefix + _serialize_table(tbl)
            docs.append(Document(text=txt, metadata=meta))
    return docs

def main():
    json_paths = process_mds_to_json(MD_DIR, json_dir=JSON_DIR, force=False)
    print(f" -> {len(json_paths)} json files ready in {JSON_DIR}")

if __name__ == "__main__":
    main()